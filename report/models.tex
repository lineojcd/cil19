\section{Models and Methods} \label{sec:models-and-methods}
In this section we describe data pre-processing techniques, model architectures and the post-processing methods that we apply to our model outputs.

\subsection{Baseline} \label{subsec:baseline}
For our baseline model, we split each of our images and masks in disjoint patches of size $16 \times 16$. We then normalize each image patch by subtracting the mean and dividing by the standard deviation across our training set. To compute the patch label, we compute the mean across the mask patch and set the label to $1$ (road) if the mean is bigger than $0.25$ and $0$ (not road) otherwise.

These patches are used to train a convolutional neural network. The exact architecture is described in \autoref{tab:baseline}.

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        Conv(filters=$32$, kernel=$5 \times 5$) & MaxPool(kernel=$2 \times 2$) & BatchNorm \\
        \hline
        \hline
        Conv(filters=$64$, kernel=$5 \times 5$) & MaxPool(kernel=$2 \times 2$) & BatchNorm \\
        \hline
        \hline
        Dense(units=$512$, activ.=ReLU) & BatchNorm &  \\
        \hline
        \hline
        Dense(units=$1$, activ.=Sigmoid) & & \\
        \hline
    \end{tabular}
    \caption{Architecture of our baseline CNN model.}
    \label{tab:baseline}
\end{table}

\subsection{Better Baseline} \label{subsec:better-baseline}
Here we borrow form the ideas presented in \cite{Mni10}. We use a much bigger patch of size $64 \times 64$ to predict the middle $16 \times 16$ patch. This gives our model more information and hence a better prediction is expected. Similarly to \autoref{subsec:baseline}, we split each image in disjoint patches of size $64 \times 64$, using $0$ padding on the edges. Image patch normalization, mask patch extraction and label computation are identical to what we did in \autoref{subsec:baseline}.

To predict the labels, we use a convolutional neural network. The exact architecture is described in \autoref{tab:better-baseline}.

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        Conv(filters=$32$, kernel=$3 \times 3$) & MaxPool(kernel=$2 \times 2$) & BatchNorm \\
        \hline
        \hline
        Conv(filters=$64$, kernel=$3 \times 3$) & MaxPool(kernel=$2 \times 2$) & BatchNorm \\
        \hline
        \hline
        Conv(filters=$64$, kernel=$3 \times 3$) & MaxPool(kernel=$2 \times 2$) & BatchNorm \\
        \hline
        \hline
        Conv(filters=$128$, kernel=$3 \times 3$) & MaxPool(kernel=$2 \times 2$) &BatchNorm \\
        \hline
        \hline
        Dense(units=$512$, activ.=ReLU) & BatchNorm &  \\
        \hline
        \hline
        Dense(units=$1$, activ.=Sigmoid) & & \\
        \hline
    \end{tabular}
    \caption{Architecture of our improved baseline CNN model.}
    \label{tab:better-baseline}
\end{table}

After obtaining the results from the CNN we tried incorporating the continuous road structure in our prediction. We group our labels into $7 \times 7$ patches and use these groupings to refine the prediction of the middle label.

We use an SVM with a radial basis function kernel as our post processing model. This different from what was done in \cite{He15} where a CNN was used.

\subsection{Fully convolutional neural network (Unet)} \label{subsec:unet}
Our best performing models in this task are fully convolutional networks. First we tried Unet as introduced in \cite{Ron15}. One of the advantages of these type of networks is that they don't need enormous amounts of data to yield excellent results.

Contrary to the baseline models, we can feed the entire image to the network and backpropagate the loss from the entire mask. Nevertheless, some pre-processing is necessary. We scale all our images to the $[0,1]$ interval and normalize the data with the channel-wise mean and standard deviation of the ImageNet \cite{Imagenet} dataset. This type of normalization is necessary because we use a model pre-trained on this particular dataset. The data is further augmented with random rotations, flips, crops and scaling.

Unet is very flexible. It allows us to try different CNN architectures in its contracting and expanding paths. Among the ones we experimented with were: VGG \cite{Zis14} and ResNet \cite{He15}.

The masks generated by the network needed further processing in order to convert them into $16 \times 16$ patch predictions. We tried averaging the model output directly in each patch and applying a threshold or applying the threshold twice, once on the pixel level and once on the mask patch average level.

\subsection{Fully convolutional neural network (FCN)} \label{subsec:fcn}
The most successful model among the ones we experimented with was the FCN based on \cite{Lon14}. Its slightly older than Unet but it shares most of its characteristics.

The preprocessing part is identical to \autoref{subsec:unet}. The model we use, is also pre-trained on the ImageNet dataset. The only addition in this phase is the random noise that we add to every image before we feed it to the model. This is done to prevent over-fitting. We noticed that this model was much more prone to over-fit than Unet.

FCN is also very flexible. We could choose from several CNN architectures in its contracting and expanding paths, pre-trained on different datasets. However, we experimented only with the ResNet variants as they proved easier to train.
