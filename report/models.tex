\section{Models and Methods} \label{sec:models-and-methods}
In this section we describe data pre-processing techniques, model architectures and the post-processing methods that we apply to our model outputs.

\subsection{Baseline} \label{subsec:baseline}
For our baseline model, we split each of our images and masks in disjoint patches of size $16 \times 16$. We then normalize each image patch by subtracting the mean and dividing by the standard deviation across our training set. To compute the patch label, we compute the mean across the mask patch and set the label to $1$ (road) if the mean is bigger than $0.25$ and $0$ (not road) otherwise.

These patches are used to train a convolutional neural network. The exact architecture is described in \autoref{tab:baseline}.

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        Conv(filters=$32$, kernel=$5 \times 5$) & MaxPool(kernel=$2 \times 2$) & BatchNorm \\
        \hline
        \hline
        Conv(filters=$64$, kernel=$5 \times 5$) & MaxPool(kernel=$2 \times 2$) & BatchNorm \\
        \hline
        \hline
        Dense(units=$512$, activ.=ReLU) & BatchNorm &  \\
        \hline
        \hline
        Dense(units=$1$, activ.=Sigmoid) & & \\
        \hline
    \end{tabular}
    \caption{Architecture of our baseline CNN model.}
    \label{tab:baseline}
\end{table}

\subsection{Improved Baseline} \label{subsec:better-baseline}
Here we borrow from the ideas presented in \cite{Mni10}. We use a much bigger patch of size $64 \times 64$ to predict the middle $16 \times 16$ patch. This gives our model more information and hence a better prediction is expected. Similarly to \autoref{subsec:baseline}, we split each image in disjoint patches of size $64 \times 64$, using $0$ padding on the edges. Image patch normalization, mask patch extraction and label computation are identical to what we did in \autoref{subsec:baseline}.

To predict the labels, we use a convolutional neural network. The exact architecture is described in \autoref{tab:better-baseline}.

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        Conv(filters=$32$, kernel=$3 \times 3$) & MaxPool(kernel=$2 \times 2$) & BatchNorm \\
        \hline
        \hline
        Conv(filters=$64$, kernel=$3 \times 3$) & MaxPool(kernel=$2 \times 2$) & BatchNorm \\
        \hline
        \hline
        Conv(filters=$64$, kernel=$3 \times 3$) & MaxPool(kernel=$2 \times 2$) & BatchNorm \\
        \hline
        \hline
        Conv(filters=$128$, kernel=$3 \times 3$) & MaxPool(kernel=$2 \times 2$) &BatchNorm \\
        \hline
        \hline
        Dense(units=$512$, activ.=ReLU) & BatchNorm &  \\
        \hline
        \hline
        Dense(units=$1$, activ.=Sigmoid) & & \\
        \hline
    \end{tabular}
    \caption{Architecture of our improved baseline CNN model.}
    \label{tab:better-baseline}
\end{table}

After obtaining the results from the CNN we tried incorporating the continuous road structure in our prediction. We group our labels into $7 \times 7$ patches and use these groupings to refine the prediction of the middle label.

We use an SVM with a radial basis function kernel as our post processing model. This different from what was done in \cite{He15} where a CNN was used.

\subsection{Fully Convolutional Neural Network (UNet)} \label{subsec:UNet}
Our best performing models in this task are fully convolutional networks, which make use of only convolutional and sampling (down \& up) layers to obtain outputs with same spatial dimensions as the input. This makes FCN an ideal candidate for tasks like segmentation. First, we try UNet as introduced in \cite{Ron15}. One of the advantages of these type of networks is that they don't need enormous amounts of data to yield excellent results.

\begin{figure}[h]
\includegraphics[width=8cm]{images/unet.png}
\caption{Auto-encoder approach using UNet. Image taken from \cite{unet}}
\end{figure}

Contrary to the baseline models, we can feed the entire image to the network and backpropagate the loss from the entire mask. Nevertheless, some pre-processing is necessary. We scale all our images to the $[0,1]$ interval and normalize the data with the channel-wise mean and standard deviation of the ImageNet \cite{Imagenet} dataset. This type of normalization is necessary because we use a model pre-trained on this particular dataset. The data is further augmented with random rotations, flips, crops and scaling. Finally, the entire model is trained end-to-end using binary cross-entropyloss at each pixel.

Being highly flexible, UNet allows us to try different CNN architectures in its contracting and expanding paths. Among the ones we experimented with were: VGG \cite{Zis14} and ResNet \cite{He15}.

The masks generated by the network needed further processing in order to convert them into $16 \times 16$ patch predictions. We tried averaging the model output directly in each patch and applying a threshold or applying the threshold twice, once on the pixel level and once on the mask patch average level.

\subsection{Fully Convolutional Neural Network (FCN)} \label{subsec:fcn}
The most successful model among the ones we experimented with was the FCN based on \cite{Lon14}. Though it is slightly older than UNet, it shares most of its characteristics.

The preprocessing part is identical to \autoref{subsec:UNet}. The model we use, is pre-trained on the MS-COCO dataset. Additionally, we augment the images as we observe that FCN is prone to overfitting. We apply random rotations, horizontal and vertical flipping, as well as random jittering in the color space, contrast, brightness etc.

FCN is also very flexible. We could choose from several CNN architectures in its contracting and expanding paths, pre-trained on different datasets. However, we experimented only with the ResNet variants as they proved easier to train.


\subsection{Deeplabv3} \label{subsec:deeplab}
Amongst the most recent models proposed for semantic segmentation, DeepLabV3 has shown promising results. DeepLab proposes to use Atrous Convolution, which helps us maximize the field of view for any given kernel without increasing the number of parameters. Using atrous convolution, the model is able to extract output features at different output strides during training and evaluation, which efficiently enables training with batch normalization. Additionally, the model also uses multiscale processing to create image pyramid along with parallel atrous convolutions. Finally, the model also makes use of a fully connected Conditional Random Fields to structure its predictions. 

\begin{figure}[h]
\includegraphics[width=8cm]{images/deeplabv2.png}
\caption{A simplistic overview of DeepLab architecture. Image taken from DeepLabv2. \cite{deeplab2}}
\end{figure}

We use a publically available implementation of the model in PyTorch, which makes use of a ResNet backbone. We finetune an already pretrained model on MS-COCO using the augmentation techniques discussed above. 

\subsection{FCN Ensemble} \label{subsec:ensemble}
In order to further improve our performance, we finally create a simple ensemble using FCN and DeepLabV3 with ResNet 101 backbones each. Similar to previous setups, we simply augment our data and train with cross entropy loss. However, we merge the final output predictions of the two models as a linear combination and normalize them, which is then trained against the target masks.
